
services:
  rtdetr-api:
    build:
      context: ./inference
      dockerfile: Dockerfile
    image: rtdetr-api:latest
    container_name: rtdetr-api
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
    ports:
      - "8000:8000"
    gpus:
      - device_ids: ["1"]
        capabilities: ["gpu"]
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      ENGINE_PATH: /app/models/rtdetr-l.engine
      NVIDIA_VISIBLE_DEVICES: "1"          # fallback for older runtimes
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    restart: unless-stopped
    networks:
      - ai-net            

  vnc-instance-1:
    image: ai-sandbox
    build:
      context: ./agent          # <- folder that contains the Dockerfile you pasted
      dockerfile: Dockerfile
    container_name: vnc-instance-1
    ports:
      - "5901:5901"
    volumes:
      - ./tasks:/tasks:rw
    gpus:
      - device_ids: ["0"]
        capabilities: ["gpu"]
    environment:
      AGENT_NAME: "agent-1"
      AGENT_TASK: "/tasks/agent-1.yaml"
      RTDETR_API_URL: "http://rtdetr-api:8000/predict"   # <-- key line
      NVIDIA_VISIBLE_DEVICES: "0"
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      TASK_PLANNER_URL: http://task-planner:8000/v1/actions/next
      PLANNER_API_KEY: taskPlannerApiSecret
    restart: unless-stopped
    depends_on:
      - rtdetr-api
    networks:
      - ai-net

  vnc-instance-2:
    image: ai-sandbox
    container_name: vnc-instance-2
    ports:
      - "5902:5901"
    volumes:
      - ./tasks:/tasks:rw
    gpus:
      - device_ids: ["0"]
        capabilities: ["gpu"]
    environment:
      AGENT_NAME: "agent-2"
      AGENT_TASK: "/tasks/agent-2.yaml"
      RTDETR_API_URL: "http://rtdetr-api:8000/predict"   # <-- key line
      NVIDIA_VISIBLE_DEVICES: "0"
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      TASK_PLANNER_URL: http://task-planner:8000/v1/actions/next
      PLANNER_API_KEY: taskPlannerApiSecret
    restart: unless-stopped
    depends_on:
      - rtdetr-api
    networks:
      - ai-net

  task-planner:
    build:
      context: ./taskPlanner
      dockerfile: Dockerfile
    container_name: task-planner
    ports:
      - "8010:8000"
    environment:
      PLANNER_API_KEY: taskPlannerApiSecret
      OLLAMA_OPENAI_BASE: http://host.docker.internal:11434/v1
      OLLAMA_MODEL: gpt-oss:120b
    # map host.docker.internal on Linux (Desktop already provides it)
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ai-net

networks:
  ai-net:
    driver: bridge
